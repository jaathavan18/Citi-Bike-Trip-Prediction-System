{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3af6ee06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading dataset...\n",
      "‚úÖ Dataset loaded successfully.\n",
      "üïí Converting datetime columns...\n",
      "‚è±Ô∏è Calculating ride duration...\n",
      "üìä Aggregating target (trip counts)...\n",
      "üîÅ Creating 112 lag features (28 days √ó 4 bins/day)...\n",
      "üìÖ Extracting time-based features...\n",
      "üßπ Dropping missing values...\n",
      "‚úÖ Preprocessing complete.\n",
      "üìà Training regression model...\n",
      "üöÄ Training LightGBM regression model...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001167 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15237\n",
      "[LightGBM] [Info] Number of data points in the train set: 664, number of used features: 116\n",
      "[LightGBM] [Info] Start training from score 99.081325\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002508 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 27808\n",
      "[LightGBM] [Info] Number of data points in the train set: 1326, number of used features: 116\n",
      "[LightGBM] [Info] Start training from score 100.471342\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002795 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28586\n",
      "[LightGBM] [Info] Number of data points in the train set: 1988, number of used features: 116\n",
      "[LightGBM] [Info] Start training from score 100.439135\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002778 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28586\n",
      "[LightGBM] [Info] Number of data points in the train set: 2650, number of used features: 116\n",
      "[LightGBM] [Info] Start training from score 101.664906\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002731 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28586\n",
      "[LightGBM] [Info] Number of data points in the train set: 3312, number of used features: 116\n",
      "[LightGBM] [Info] Start training from score 103.574879\n",
      "\n",
      "üìä Evaluation Metrics (5-fold CV):\n",
      "‚û°Ô∏è  Mean Absolute Error (MAE): 22.67\n",
      "‚û°Ô∏è  Mean Absolute Percentage Error (MAPE): 58.70%\n",
      "‚û°Ô∏è  Root Mean Squared Error (RMSE): 33.39\n",
      "‚û°Ô∏è  R-squared (R¬≤): 0.83\n",
      "üìå Calculating feature importance...\n",
      "\n",
      "üèÜ Top 5 Important Features:\n",
      "           feature  importance\n",
      "0     target_lag_1         185\n",
      "3     target_lag_4         136\n",
      "113    day_of_week          98\n",
      "1     target_lag_2          65\n",
      "27   target_lag_28          57\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    print(\"üì• Loading dataset...\")\n",
    "    df = pd.read_parquet(file_path)\n",
    "    print(\"‚úÖ Dataset loaded successfully.\")\n",
    "\n",
    "    print(\"üïí Converting datetime columns...\")\n",
    "    df['started_at'] = pd.to_datetime(df['started_at'], format='mixed')\n",
    "    df['ended_at'] = pd.to_datetime(df['ended_at'], format='mixed')  \n",
    "    df['pickup_hour'] = df['started_at'].dt.floor('6H')\n",
    "    df['location_id'] = df['start_station_id'].astype(str)\n",
    "\n",
    "    print(\"‚è±Ô∏è Calculating ride duration...\")\n",
    "    df['duration_minutes'] = (df['ended_at'] - df['started_at']).dt.total_seconds() / 60.0\n",
    "\n",
    "    print(\"üìä Aggregating target (trip counts)...\")\n",
    "    ride_counts = df.groupby(['pickup_hour', 'location_id']).size().reset_index(name='target')\n",
    "\n",
    "    print(\"üîÅ Creating 112 lag features (28 days √ó 4 bins/day)...\")\n",
    "    lagged_data = []\n",
    "    for loc in ride_counts['location_id'].unique():\n",
    "        loc_df = ride_counts[ride_counts['location_id'] == loc].sort_values('pickup_hour')\n",
    "        for lag in range(1, 113):\n",
    "            loc_df[f'target_lag_{lag}'] = loc_df['target'].shift(lag)\n",
    "        lagged_data.append(loc_df)\n",
    "\n",
    "    df_lagged = pd.concat(lagged_data)\n",
    "\n",
    "    print(\"üìÖ Extracting time-based features...\")\n",
    "    df_lagged['hour'] = df_lagged['pickup_hour'].dt.hour\n",
    "    df_lagged['day_of_week'] = df_lagged['pickup_hour'].dt.dayofweek\n",
    "    df_lagged['month'] = df_lagged['pickup_hour'].dt.month\n",
    "    df_lagged['is_weekend'] = df_lagged['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "    print(\"üßπ Dropping missing values...\")\n",
    "    df_lagged = df_lagged.dropna()\n",
    "\n",
    "    print(\"‚úÖ Preprocessing complete.\")\n",
    "    return df_lagged\n",
    "\n",
    "def train_lgb_regression_model(df):\n",
    "    features = [col for col in df.columns if col not in ['pickup_hour', 'target', 'location_id']]\n",
    "    X = df[features]\n",
    "    y = df['target']\n",
    "\n",
    "    model = lgb.LGBMRegressor(random_state=42)\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    rmse_scores, mae_scores, r2_scores, mape_scores = [], [], [], []\n",
    "\n",
    "    print(\"üöÄ Training LightGBM regression model...\")\n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "        mae_scores.append(mean_absolute_error(y_test, y_pred))\n",
    "        r2_scores.append(r2_score(y_test, y_pred))\n",
    "        mape_scores.append(np.mean(np.abs((y_test - y_pred) / y_test)) * 100)\n",
    "\n",
    "    print(\"\\nüìä Evaluation Metrics (5-fold CV):\")\n",
    "    print(f\"‚û°Ô∏è  Mean Absolute Error (MAE): {np.mean(mae_scores):.2f}\")\n",
    "    print(f\"‚û°Ô∏è  Mean Absolute Percentage Error (MAPE): {np.mean(mape_scores):.2f}%\")\n",
    "    print(f\"‚û°Ô∏è  Root Mean Squared Error (RMSE): {np.mean(rmse_scores):.2f}\")\n",
    "    print(f\"‚û°Ô∏è  R-squared (R¬≤): {np.mean(r2_scores):.2f}\")\n",
    "\n",
    "    return model, features\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"BikeRide2024Top3Location.parquet\"\n",
    "    \n",
    "    df_transformed = load_and_preprocess_data(file_path)\n",
    "    df_transformed.to_parquet(\"transformeddata2024.parquet\", index=False)\n",
    "    \n",
    "    print(\"üìà Training regression model...\")\n",
    "    reg_model, reg_features = train_lgb_regression_model(df_transformed)\n",
    "    \n",
    "    print(\"üìå Calculating feature importance...\")\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': reg_features,\n",
    "        'importance': reg_model.feature_importances_\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "\n",
    "    print(\"\\nüèÜ Top 5 Important Features:\")\n",
    "    print(feature_importance.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d33a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class BaselineModelPreviousHour:\n",
    "    \"\"\"\n",
    "    A simple baseline model that uses the previous time step's value (e.g., rides_t-1)\n",
    "    as the prediction for the current time step.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X_train: pd.DataFrame, y_train: pd.Series):\n",
    "        # No training needed for baseline model\n",
    "        pass\n",
    "\n",
    "    def predict(self, X_test: pd.DataFrame) -> np.ndarray:\n",
    "        if \"target_lag_1\" not in X_test.columns:\n",
    "            raise ValueError(\"X_test must contain 'target_lag_1' column.\")\n",
    "        return X_test[\"target_lag_1\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5d8942e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä BaselineModelPreviousHour Evaluation:\n",
      "‚û°Ô∏è MAE:  103.48\n",
      "‚û°Ô∏è MAPE: 485.11%\n",
      "‚û°Ô∏è RMSE: 115.55\n",
      "‚û°Ô∏è R¬≤:   -0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load transformed data\n",
    "df = pd.read_parquet(\"transformeddata2024.parquet\")\n",
    "\n",
    "# Define features and target\n",
    "# features = [col for col in df.columns if col not in ['Pickup_hour', 'target', 'location_id']]\n",
    "features = [col for col in df.columns if col not in ['pickup_hour', 'target', 'location_id']]\n",
    "X = df[features]\n",
    "y = df['target']\n",
    "\n",
    "# Simulate train/test split (use last 20% as test)\n",
    "split_idx = int(0.8 * len(df))\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "# Baseline model\n",
    "baseline = BaselineModelPreviousHour()\n",
    "baseline.fit(X_train, y_train)\n",
    "y_pred = baseline.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "\n",
    "print(\"üìä BaselineModelPreviousHour Evaluation:\")\n",
    "print(f\"‚û°Ô∏è MAE:  {mae:.2f}\")\n",
    "print(f\"‚û°Ô∏è MAPE: {mape:.2f}%\")\n",
    "print(f\"‚û°Ô∏è RMSE: {rmse:.2f}\")\n",
    "print(f\"‚û°Ô∏è R¬≤:   {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b8990c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_lag_1      float64\n",
      "target_lag_2      float64\n",
      "target_lag_3      float64\n",
      "target_lag_4      float64\n",
      "target_lag_5      float64\n",
      "                   ...   \n",
      "target_lag_112    float64\n",
      "hour                int32\n",
      "day_of_week         int32\n",
      "month               int32\n",
      "is_weekend          int32\n",
      "Length: 116, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa40bd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003294 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28586\n",
      "[LightGBM] [Info] Number of data points in the train set: 3179, number of used features: 116\n",
      "[LightGBM] [Info] Start training from score 102.463982\n",
      "üìä LightGBM Model Evaluation:\n",
      "‚û°Ô∏è MAE:  20.92\n",
      "‚û°Ô∏è MAPE: 41.63%\n",
      "‚û°Ô∏è RMSE: 30.53\n",
      "‚û°Ô∏è R¬≤:   0.88\n",
      "\n",
      "üèÜ Top 5 Feature Importances:\n",
      "           feature  importance\n",
      "0     target_lag_1         185\n",
      "3     target_lag_4         140\n",
      "113    day_of_week          90\n",
      "1     target_lag_2          68\n",
      "27   target_lag_28          54\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class LightGBMRegressorModel:\n",
    "    \"\"\"\n",
    "    A wrapper around LightGBM Regressor for consistent interface.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.model = lgb.LGBMRegressor(random_state=42, **kwargs)\n",
    "\n",
    "    def fit(self, X_train: pd.DataFrame, y_train: pd.Series):\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test: pd.DataFrame) -> np.ndarray:\n",
    "        return self.model.predict(X_test)\n",
    "\n",
    "    def feature_importance(self, feature_names: list) -> pd.DataFrame:\n",
    "        return pd.DataFrame({\n",
    "            \"feature\": feature_names,\n",
    "            \"importance\": self.model.feature_importances_\n",
    "        }).sort_values(by=\"importance\", ascending=False)\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load transformed data\n",
    "df = pd.read_parquet(\"transformeddata2024.parquet\")\n",
    "\n",
    "# Define features and target\n",
    "features = [col for col in df.columns if col not in ['pickup_hour', 'target', 'location_id']]\n",
    "X = df[features]\n",
    "y = df['target']\n",
    "\n",
    "# Train/test split\n",
    "split_idx = int(0.8 * len(df))\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "# Train LightGBM model\n",
    "lgb_model = LightGBMRegressorModel()\n",
    "lgb_model.fit(X_train, y_train)\n",
    "y_pred = lgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "\n",
    "print(\"üìä LightGBM Model Evaluation:\")\n",
    "print(f\"‚û°Ô∏è MAE:  {mae:.2f}\")\n",
    "print(f\"‚û°Ô∏è MAPE: {mape:.2f}%\")\n",
    "print(f\"‚û°Ô∏è RMSE: {rmse:.2f}\")\n",
    "print(f\"‚û°Ô∏è R¬≤:   {r2:.2f}\")\n",
    "\n",
    "# Feature importance\n",
    "print(\"\\nüèÜ Top 5 Feature Importances:\")\n",
    "print(lgb_model.feature_importance(features).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41dcfe78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model saved to 'lightgbm_bikeride_model.joblib'\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained LightGBM model to a file\n",
    "joblib.dump(lgb_model.model, \"lightgbm_bikeride_model.joblib\")\n",
    "print(\"‚úÖ Model saved to 'lightgbm_bikeride_model.joblib'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
