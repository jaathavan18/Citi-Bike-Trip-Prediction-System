{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a92b6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "930b869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_lgb_regression_model(df):\n",
    "    features = [col for col in df.columns if col not in ['pickup_hour', 'target', 'location_id']]\n",
    "    X = df[features]\n",
    "    y = df['target']\n",
    "\n",
    "    model = lgb.LGBMRegressor(random_state=42)\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    rmse_scores, mae_scores, r2_scores, mape_scores = [], [], [], []\n",
    "\n",
    "    print(\"üöÄ Training LightGBM regression model...\")\n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        dashhubmod=model\n",
    "\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "        mae_scores.append(mean_absolute_error(y_test, y_pred))\n",
    "        r2_scores.append(r2_score(y_test, y_pred))\n",
    "        mape_scores.append(np.mean(np.abs((y_test - y_pred) / y_test)) * 100)\n",
    "\n",
    "    print(\"\\nüìä Evaluation Metrics (5-fold CV):\")\n",
    "    print(f\"‚û°Ô∏è  Mean Absolute Error (MAE): {np.mean(mae_scores):.2f}\")\n",
    "    print(f\"‚û°Ô∏è  Mean Absolute Percentage Error (MAPE): {np.mean(mape_scores):.2f}%\")\n",
    "    print(f\"‚û°Ô∏è  Root Mean Squared Error (RMSE): {np.mean(rmse_scores):.2f}\")\n",
    "    print(f\"‚û°Ô∏è  R-squared (R¬≤): {np.mean(r2_scores):.2f}\")\n",
    "\n",
    "    return model, features,X_test,rmse_scores,mae_scores,r2_scores,mape_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "618533c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Training regression model...\n",
      "üöÄ Training LightGBM regression model...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001349 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15237\n",
      "[LightGBM] [Info] Number of data points in the train set: 664, number of used features: 116\n",
      "[LightGBM] [Info] Start training from score 99.081325\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002551 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 27808\n",
      "[LightGBM] [Info] Number of data points in the train set: 1326, number of used features: 116\n",
      "[LightGBM] [Info] Start training from score 100.471342\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002914 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28586\n",
      "[LightGBM] [Info] Number of data points in the train set: 1988, number of used features: 116\n",
      "[LightGBM] [Info] Start training from score 100.439135\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002797 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28586\n",
      "[LightGBM] [Info] Number of data points in the train set: 2650, number of used features: 116\n",
      "[LightGBM] [Info] Start training from score 101.664906\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002676 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28586\n",
      "[LightGBM] [Info] Number of data points in the train set: 3312, number of used features: 116\n",
      "[LightGBM] [Info] Start training from score 103.574879\n",
      "\n",
      "üìä Evaluation Metrics (5-fold CV):\n",
      "‚û°Ô∏è  Mean Absolute Error (MAE): 22.67\n",
      "‚û°Ô∏è  Mean Absolute Percentage Error (MAPE): 58.70%\n",
      "‚û°Ô∏è  Root Mean Squared Error (RMSE): 33.39\n",
      "‚û°Ô∏è  R-squared (R¬≤): 0.83\n",
      "üìå Calculating feature importance...\n",
      "\n",
      "üèÜ Top 5 Important Features:\n",
      "           feature  importance\n",
      "0     target_lag_1         185\n",
      "3     target_lag_4         136\n",
      "113    day_of_week          98\n",
      "1     target_lag_2          65\n",
      "27   target_lag_28          57\n"
     ]
    }
   ],
   "source": [
    "print(\"üìà Training regression model...\")\n",
    "df_transformed=pd.read_parquet(\"transformeddata2024.parquet\")\n",
    "reg_model, reg_features,X_test,rmse_scores,mae_scores,r2_scores,mape_scores = train_lgb_regression_model(df_transformed)\n",
    "    \n",
    "print(\"üìå Calculating feature importance...\")\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': reg_features,\n",
    "    'importance': reg_model.feature_importances_\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(\"\\nüèÜ Top 5 Important Features:\")\n",
    "print(feature_importance.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94061723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def set_mlflow_tracking():\n",
    "    \"\"\"\n",
    "    Set up MLflow tracking server credentials and URI.\n",
    "    \"\"\"\n",
    "    uri = os.environ[\"MLFLOW_TRACKING_URI\"]\n",
    "    print(uri)\n",
    "    mlflow.set_tracking_uri(uri)\n",
    "    logger.info(\"MLflow tracking URI and credentials set.\")\n",
    "\n",
    "    return mlflow\n",
    "\n",
    "\n",
    "def log_model_to_mlflow(\n",
    "     model,\n",
    "    input_data,\n",
    "    experiment_name,\n",
    "    metric_name=\"metric\",\n",
    "    model_name=None,\n",
    "    params=None,\n",
    "    mae=None,\n",
    "    mape=None,\n",
    "    rmse=None,\n",
    "    r2=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Log a trained model, parameters, and metrics to MLflow.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained model object (e.g., sklearn model).\n",
    "    - input_data: Input data used for training (for signature inference).\n",
    "    - experiment_name: Name of the MLflow experiment.\n",
    "    - metric_name: Name of the metric to log (e.g., \"RMSE\", \"accuracy\").\n",
    "    - model_name: Optional name for the registered model.\n",
    "    - params: Optional dictionary of hyperparameters to log.\n",
    "    - score: Optional evaluation metric to log.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set the experiment\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        logger.info(f\"Experiment set to: {experiment_name}\")\n",
    "\n",
    "        # Start an MLflow run\n",
    "        with mlflow.start_run():\n",
    "            # Log hyperparameters if provided\n",
    "            if params:\n",
    "                mlflow.log_params(params)\n",
    "                logger.info(f\"Logged parameters: {params}\")\n",
    "\n",
    "            # Log metrics if provided\n",
    "            if mae is not None:\n",
    "                mlflow.log_metric(metric_name, mae)\n",
    "                mlflow.log_metric(\"mape\", mape)\n",
    "                mlflow.log_metric(\"rmse\", rmse)\n",
    "                mlflow.log_metric(\"r2\", r2)\n",
    "                logger.info(f\"Logged {metric_name}: {mae}\")\n",
    "\n",
    "            # Infer the model signature\n",
    "            signature = infer_signature(input_data, model.predict(input_data))\n",
    "            logger.info(\"Model signature inferred.\")\n",
    "\n",
    "            # Determine the model name\n",
    "            if not model_name:\n",
    "                model_name = model.__class__.__name__\n",
    "\n",
    "            # Log the model\n",
    "            model_info = mlflow.sklearn.log_model(\n",
    "                sk_model=model,\n",
    "                artifact_path=\"model_artifact\",\n",
    "                signature=signature,\n",
    "                input_example=input_data,\n",
    "                registered_model_name=model_name,\n",
    "            )\n",
    "            logger.info(f\"Model logged with name: {model_name}\")\n",
    "            return model_info\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while logging to MLflow: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c24b644",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:MLflow tracking URI and credentials set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://dagshub.com/jaathavan18/citi_bike_pred.mlflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/11 11:55:13 INFO mlflow.tracking.fluent: Experiment with name 'LightGbmWith5foldModel' does not exist. Creating a new experiment.\n",
      "INFO:__main__:Experiment set to: LightGbmWith5foldModel\n",
      "INFO:__main__:Logged mean_absolute_error: 22.67\n",
      "c:\\Users\\Jaath\\anaconda3\\envs\\citienv\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "INFO:__main__:Model signature inferred.\n",
      "c:\\Users\\Jaath\\anaconda3\\envs\\citienv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 2339.27it/s]\n",
      "2025/05/11 11:55:20 INFO mlflow.models.model: Found the following environment variables used during model inference: [HOPSWORKS_API_KEY]. Please check if you need to set them when deploying the model. To disable this message, set environment variable `MLFLOW_RECORD_ENV_VARS_IN_MODEL_LOGGING` to `false`.\n",
      "Registered model 'LGBMRegressor' already exists. Creating a new version of this model...\n",
      "2025/05/11 11:55:26 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: LGBMRegressor, version 5\n",
      "Created version '5' of model 'LGBMRegressor'.\n",
      "INFO:__main__:Model logged with name: LGBMRegressor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run able-gnat-78 at: https://dagshub.com/jaathavan18/citi_bike_pred.mlflow/#/experiments/9/runs/fd0e8c4dbba748279fdab0f1ab91cd0a\n",
      "üß™ View experiment at: https://dagshub.com/jaathavan18/citi_bike_pred.mlflow/#/experiments/9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlflow.models.model.ModelInfo at 0x1dc1bb624d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "\n",
    "mlflow = set_mlflow_tracking()\n",
    "log_model_to_mlflow(model=reg_model,\n",
    "    input_data=X_test,\n",
    "    experiment_name=\"LightGbmWith5foldModel\",\n",
    "    metric_name=\"mean_absolute_error\",\n",
    "    mae=round(np.mean(mae_scores), 2),\n",
    "mape=round(np.mean(mape_scores), 2),\n",
    "rmse=round(np.mean(rmse_scores), 2),\n",
    "r2=round(np.mean(r2_scores), 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
